"""
prepare_rppg_dataset.py - rPPG ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¶„í• 

Phase 3-1: Domain Adaptation ì¤€ë¹„
ëª©í‘œ: PPG ëª¨ë¸ì„ rPPG ë°ì´í„°ë¡œ fine-tuningí•˜ê¸° ìœ„í•œ ë°ì´í„° ì¤€ë¹„

ì‘ì—…:
1. rPPG-BP-UKL_rppg_7s.h5 ë¡œë“œ
2. Train/Val/Test split (70/15/15)
3. ì •ê·œí™”
4. ë¶„í•  ë°ì´í„° ì €ì¥
"""

import os
import h5py
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import argparse


def load_rppg_dataset(data_path):
    """
    rPPG ë°ì´í„°ì…‹ ë¡œë“œ
    
    Args:
        data_path: rPPG-BP-UKL_rppg_7s.h5 ê²½ë¡œ
    
    Returns:
        signals: (N, 875) - rPPG ì‹ í˜¸
        labels: (N, 2) - [SBP, DBP]
    """
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    
    with h5py.File(data_path, 'r') as f:
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        print(f"   ë°ì´í„°ì…‹ í‚¤: {list(f.keys())}")
        
        # rPPG-BP-UKL êµ¬ì¡°: label (2, N), rppg (875, N), subject_idx (1, N)
        # ì—¬ê¸°ì„œ Nì€ ìƒ˜í”Œ ìˆ˜
        labels_raw = f['label'][:]  # (2, N) - [SBP, DBP]
        signals_raw = f['rppg'][:]  # (875, N) - ì‹ í˜¸ ê¸¸ì´ Ã— ìƒ˜í”Œ
        
        # ì „ì¹˜: (2, N) â†’ (N, 2), (875, N) â†’ (N, 875)
        labels = labels_raw.T  # (N, 2)
        signals = signals_raw.T  # (N, 875)
    
    print(f"   rPPG ì‹ í˜¸ í˜•íƒœ: {signals.shape} (ìƒ˜í”Œ ìˆ˜, ì‹ í˜¸ ê¸¸ì´)")
    print(f"   SBP/DBP ë ˆì´ë¸” í˜•íƒœ: {labels.shape} (ìƒ˜í”Œ ìˆ˜, 2)")
    
    return signals, labels


def validate_data(signals, labels):
    """
    ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    
    Args:
        signals: (N, 875)
        labels: (N, 2)
    """
    print("\nâœ“ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬")
    
    # ê²°ì¸¡ì¹˜ í™•ì¸
    nan_signals = np.sum(np.isnan(signals))
    nan_labels = np.sum(np.isnan(labels))
    print(f"   ì‹ í˜¸ NaN: {nan_signals}, ë ˆì´ë¸” NaN: {nan_labels}")
    
    if nan_signals > 0 or nan_labels > 0:
        # NaN ì œê±°
        valid_idx = ~(np.isnan(signals).any(axis=1) | np.isnan(labels).any(axis=1))
        signals = signals[valid_idx]
        labels = labels[valid_idx]
        print(f"   NaN ì œê±° í›„: signals {signals.shape}, labels {labels.shape}")
    
    # í˜ˆì•• ë²”ìœ„ í™•ì¸ (ì´ìƒì¹˜ ì œê±°)
    sbp, dbp = labels[:, 0], labels[:, 1]
    print(f"   SBP ë²”ìœ„: {sbp.min():.1f} - {sbp.max():.1f} mmHg")
    print(f"   DBP ë²”ìœ„: {dbp.min():.1f} - {dbp.max():.1f} mmHg")
    
    # ë¹„ì •ìƒ ë²”ìœ„ ì œê±° (ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ê°’)
    valid_idx = (sbp >= 50) & (sbp <= 250) & (dbp >= 20) & (dbp <= 150) & (sbp > dbp)
    signals = signals[valid_idx]
    labels = labels[valid_idx]
    print(f"   ì´ìƒì¹˜ ì œê±° í›„: signals {signals.shape}, labels {labels.shape}")
    
    return signals, labels


def normalize_data(signals, labels):
    """
    ë°ì´í„° ì •ê·œí™”
    
    Args:
        signals: (N, 875)
        labels: (N, 2)
    
    Returns:
        signals_normalized: (N, 875) - ì •ê·œí™”ëœ ì‹ í˜¸
        labels_normalized: (N, 2) - ì •ê·œí™”ëœ ë ˆì´ë¸”
        signal_scaler: StandardScaler (ì‹ í˜¸ìš©)
        label_scaler: StandardScaler (ë ˆì´ë¸”ìš©)
    """
    print("\nâš™ï¸ ì •ê·œí™” ì¤‘...")
    
    # ì‹ í˜¸ ì •ê·œí™” (ê° ìƒ˜í”Œë³„ z-score)
    signal_scaler = StandardScaler()
    signals_normalized = signal_scaler.fit_transform(signals)
    print(f"   ì‹ í˜¸ ì •ê·œí™” ì™„ë£Œ")
    print(f"   í‰ê· : {signals_normalized.mean():.4f}, í‘œì¤€í¸ì°¨: {signals_normalized.std():.4f}")
    
    # ë ˆì´ë¸” ì •ê·œí™”
    label_scaler = StandardScaler()
    labels_normalized = label_scaler.fit_transform(labels)
    print(f"   ë ˆì´ë¸” ì •ê·œí™” ì™„ë£Œ")
    print(f"   SBP - í‰ê· : {labels_normalized[:, 0].mean():.4f}, í‘œì¤€í¸ì°¨: {labels_normalized[:, 0].std():.4f}")
    print(f"   DBP - í‰ê· : {labels_normalized[:, 1].mean():.4f}, í‘œì¤€í¸ì°¨: {labels_normalized[:, 1].std():.4f}")
    
    return signals_normalized, labels_normalized, signal_scaler, label_scaler


def split_dataset(signals, labels, train_ratio=0.7, val_ratio=0.15):
    """
    ë°ì´í„°ì…‹ ë¶„í• 
    
    Args:
        signals: (N, 875)
        labels: (N, 2)
        train_ratio: í•™ìŠµ ë¹„ìœ¨
        val_ratio: ê²€ì¦ ë¹„ìœ¨ (ë‚˜ë¨¸ì§€ëŠ” í…ŒìŠ¤íŠ¸)
    
    Returns:
        train_signals, val_signals, test_signals
        train_labels, val_labels, test_labels
    """
    print(f"\nâœ‚ï¸ ë°ì´í„°ì…‹ ë¶„í•  (Train:{train_ratio*100}%, Val:{val_ratio*100}%, Test:{(1-train_ratio-val_ratio)*100}%)")
    
    # ì²« ë²ˆì§¸ ë¶„í• : train vs (val+test)
    train_signals, temp_signals, train_labels, temp_labels = train_test_split(
        signals, labels, 
        test_size=(1-train_ratio), 
        random_state=42
    )
    
    # ë‘ ë²ˆì§¸ ë¶„í• : val vs test
    val_size = val_ratio / (1 - train_ratio)
    val_signals, test_signals, val_labels, test_labels = train_test_split(
        temp_signals, temp_labels,
        test_size=(1-val_size),
        random_state=42
    )
    
    print(f"   Train: {train_signals.shape[0]} ({train_signals.shape[0]/len(signals)*100:.1f}%)")
    print(f"   Val:   {val_signals.shape[0]} ({val_signals.shape[0]/len(signals)*100:.1f}%)")
    print(f"   Test:  {test_signals.shape[0]} ({test_signals.shape[0]/len(signals)*100:.1f}%)")
    
    return (train_signals, val_signals, test_signals,
            train_labels, val_labels, test_labels)


def save_split_dataset(output_dir, 
                       train_signals, val_signals, test_signals,
                       train_labels, val_labels, test_labels,
                       signal_scaler, label_scaler):
    """
    ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥
    
    Args:
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        train/val/test_signals: ì‹ í˜¸
        train/val/test_labels: ë ˆì´ë¸”
        signal_scaler: ì‹ í˜¸ ìŠ¤ì¼€ì¼ëŸ¬
        label_scaler: ë ˆì´ë¸” ìŠ¤ì¼€ì¼ëŸ¬
    """
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Train ë°ì´í„°ì…‹
    train_file = os.path.join(output_dir, 'rppg_train.h5')
    with h5py.File(train_file, 'w') as f:
        f.create_dataset('signals', data=train_signals)
        f.create_dataset('labels', data=train_labels)
        f.attrs['split'] = 'train'
        f.attrs['count'] = len(train_signals)
    print(f"   âœ“ {train_file} ({len(train_signals)} ìƒ˜í”Œ)")
    
    # Val ë°ì´í„°ì…‹
    val_file = os.path.join(output_dir, 'rppg_val.h5')
    with h5py.File(val_file, 'w') as f:
        f.create_dataset('signals', data=val_signals)
        f.create_dataset('labels', data=val_labels)
        f.attrs['split'] = 'val'
        f.attrs['count'] = len(val_signals)
    print(f"   âœ“ {val_file} ({len(val_signals)} ìƒ˜í”Œ)")
    
    # Test ë°ì´í„°ì…‹
    test_file = os.path.join(output_dir, 'rppg_test.h5')
    with h5py.File(test_file, 'w') as f:
        f.create_dataset('signals', data=test_signals)
        f.create_dataset('labels', data=test_labels)
        f.attrs['split'] = 'test'
        f.attrs['count'] = len(test_signals)
    print(f"   âœ“ {test_file} ({len(test_signals)} ìƒ˜í”Œ)")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì €ì¥
    info_file = os.path.join(output_dir, 'rppg_info.txt')
    with open(info_file, 'w') as f:
        f.write("rPPG Dataset Information\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Total samples: {len(train_signals) + len(val_signals) + len(test_signals)}\n")
        f.write(f"Train: {len(train_signals)} samples\n")
        f.write(f"Val: {len(val_signals)} samples\n")
        f.write(f"Test: {len(test_signals)} samples\n\n")
        f.write("Signal Statistics:\n")
        f.write(f"  Mean: {signal_scaler.mean_}\n")
        f.write(f"  Scale: {signal_scaler.scale_}\n\n")
        f.write("Label Statistics:\n")
        f.write(f"  Mean: {label_scaler.mean_}\n")
        f.write(f"  Scale: {label_scaler.scale_}\n")
    print(f"   âœ“ {info_file}")


def main():
    parser = argparse.ArgumentParser(description='rPPG ë°ì´í„°ì…‹ ì „ì²˜ë¦¬')
    parser.add_argument('--input', type=str, 
                       default='data/rPPG-BP-UKL_rppg_7s.h5',
                       help='ì…ë ¥ ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--output', type=str,
                       default='data',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--train-ratio', type=float, default=0.7,
                       help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨')
    parser.add_argument('--val-ratio', type=float, default=0.15,
                       help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("rPPG ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¶„í• ")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    signals, labels = load_rppg_dataset(args.input)
    print(f"   âœ“ ë¡œë“œ ì™„ë£Œ: {len(signals)} ìƒ˜í”Œ")
    
    # 2. ìœ íš¨ì„± ê²€ì‚¬
    signals, labels = validate_data(signals, labels)
    print(f"   âœ“ ìœ íš¨ì„± ê²€ì‚¬ ì™„ë£Œ: {len(signals)} ìƒ˜í”Œ")
    
    # 3. ì •ê·œí™”
    signals_norm, labels_norm, signal_scaler, label_scaler = normalize_data(signals, labels)
    
    # 4. ë°ì´í„°ì…‹ ë¶„í• 
    (train_signals, val_signals, test_signals,
     train_labels, val_labels, test_labels) = split_dataset(
        signals_norm, labels_norm,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio
    )
    
    # 5. ì €ì¥
    save_split_dataset(
        args.output,
        train_signals, val_signals, test_signals,
        train_labels, val_labels, test_labels,
        signal_scaler, label_scaler
    )
    
    print("\n" + "="*60)
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("="*60)
    print(f"\në‹¤ìŒ ë‹¨ê³„: domain_adaptation.py í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print(f"   python domain_adaptation.py")


if __name__ == '__main__':
    main()
