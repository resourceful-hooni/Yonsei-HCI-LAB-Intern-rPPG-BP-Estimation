# ğŸ©º rPPG ê¸°ë°˜ í˜ˆì•• ì˜ˆì¸¡ - ì¢…í•© ë¬¸ì œ ë¶„ì„ ë° í•´ê²° ê°€ì´ë“œ

**ì‘ì„±ì¼:** 2026-01-19  
**í”„ë¡œì íŠ¸:** Non-invasive Blood Pressure Estimation Using Deep Learning  
**ë…¼ë¬¸:** Schrumpf et al. 2021 - "Assessment of Non-Invasive Blood Pressure Prediction from PPG and rPPG Signals Using Deep Learning"

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ë¬¸ì œì  ì „ì²´ ì •ë¦¬](#1-í˜„ì¬-ë¬¸ì œì -ì „ì²´-ì •ë¦¬)
2. [ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•](#2-ë‹¨ê³„ë³„-í•´ê²°-ë°©ë²•)
3. [ë…¼ë¬¸ ê¸°ë°˜ ì •í™•í•œ êµ¬í˜„](#3-ë…¼ë¬¸-ê¸°ë°˜-ì •í™•í•œ-êµ¬í˜„)
4. [2026ë…„ ìµœì‹  ë°©ë²•ë¡ ](#4-2026ë…„-ìµœì‹ -ë°©ë²•ë¡ )
5. [êµ¬í˜„ ë¡œë“œë§µ](#5-êµ¬í˜„-ë¡œë“œë§µ)
6. [ì„±ëŠ¥ ë¹„êµí‘œ](#6-ì„±ëŠ¥-ë¹„êµí‘œ)
7. [ì½”ë“œ ì˜ˆì œ](#7-ì½”ë“œ-ì˜ˆì œ)
8. [ì°¸ê³  ìë£Œ](#8-ì°¸ê³ -ìë£Œ)

---

## 1. í˜„ì¬ ë¬¸ì œì  ì „ì²´ ì •ë¦¬

### 1.1 ë¬¸ì œì  ì‹¬ê°ë„ ë¶„ë¥˜

| ì‹¬ê°ë„ | ë¬¸ì œ | í˜„ì¬ ìƒíƒœ | ì˜í–¥ |
|--------|------|-----------|------|
| ğŸ”´ **Critical** | POS ì•Œê³ ë¦¬ì¦˜ ë¯¸êµ¬í˜„ | Green ì±„ë„ í‰ê· ë§Œ ì‚¬ìš© | ì‹ í˜¸ í’ˆì§ˆ ë§¤ìš° ë‚®ìŒ |
| ğŸ”´ **Critical** | ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì—†ìŒ | í•„í„°ë§ ì „ë¬´ | ë…¸ì´ì¦ˆ ì œê±° ì•ˆë¨ |
| ğŸŸ  **High** | Haar Cascade ë¶€ì •í™• | ë‹¤ì¤‘ ê°ì§€, ì˜¤íƒì§€ | ì˜ëª»ëœ ROI |
| ğŸŸ  **High** | ëª¨ë¸-ë°ì´í„° ë¶ˆì¼ì¹˜ | PPG ëª¨ë¸ì— rPPG ì…ë ¥ | ì˜ˆì¸¡ê°’ ë¹„ì •ìƒ |
| ğŸŸ¡ **Medium** | ë¦¬ìƒ˜í”Œë§ ë°©ì‹ | ë‹¨ìˆœ ì„ í˜• ë³´ê°„ | ì£¼íŒŒìˆ˜ ì •ë³´ ì†ì‹¤ |
| ğŸŸ¡ **Medium** | ì›€ì§ì„/ì¡°ëª… ë³´ì • ì—†ìŒ | ë³´ì • ì „ë¬´ | í™˜ê²½ ë³€í™”ì— ì·¨ì•½ |

### 1.2 ìƒì„¸ ë¬¸ì œ ë¶„ì„

#### ğŸ”´ Critical Issue 1: POS ì•Œê³ ë¦¬ì¦˜ ë¯¸êµ¬í˜„

**í˜„ì¬ êµ¬í˜„:**
```python
# camera_rppg_test.py - í˜„ì¬ ë°©ì‹
green_channel = face_region[:, :, 1]  # BGRì—ì„œ Greenë§Œ
signal_value = np.mean(green_channel)  # ë‹¨ìˆœ í‰ê· 
```

**ë¬¸ì œì :**
- Green ì±„ë„ì€ í˜ˆë¥˜ ì™¸ì—ë„ ì¡°ëª…, ê·¸ë¦¼ì, ì›€ì§ì„ ëª¨ë‘ í¬í•¨
- í”¼ë¶€ ë°˜ì‚¬ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- SNR (Signal-to-Noise Ratio) ë§¤ìš° ë‚®ìŒ

**ë…¼ë¬¸ì˜ POS ì•Œê³ ë¦¬ì¦˜:**
```
RGB ì •ê·œí™” â†’ ì§êµ íˆ¬ì˜ â†’ í„ìŠ¤ ì‹ í˜¸ ë¶„ë¦¬
```

#### ğŸ”´ Critical Issue 2: ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì—†ìŒ

**í˜„ì¬ êµ¬í˜„:**
```python
# ì •ê·œí™”ë§Œ ìˆ˜í–‰
signal = (signal - np.mean(signal)) / np.std(signal)
```

**ë¬¸ì œì :**
- ì‹¬ë°•ìˆ˜ ë²”ìœ„: 0.7-4 Hz (42-240 bpm)
- ì´ ë²”ìœ„ ì™¸ì˜ ë…¸ì´ì¦ˆê°€ ëª¨ë‘ í¬í•¨ë¨
- í˜¸í¡ (0.1-0.5 Hz), ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ë“± ì œê±° ì•ˆë¨

#### ğŸŸ  High Issue 3: Haar Cascade ë¶€ì •í™•

**í˜„ì¬ êµ¬í˜„:**
```python
faces = face_cascade.detectMultiScale(gray, 1.1, 4)
```

**ë¬¸ì œì :**
- `minNeighbors=4`ê°€ ë„ˆë¬´ ë‚®ìŒ â†’ ë‹¤ì¤‘ ê°ì§€
- ì–¼êµ´ì´ ì•„ë‹Œ ì˜ì—­ ì˜¤íƒì§€
- í”„ë ˆì„ë§ˆë‹¤ ROI ìœ„ì¹˜ ë³€ë™

#### ğŸŸ  High Issue 4: ëª¨ë¸-ë°ì´í„° ë¶ˆì¼ì¹˜

**í•™ìŠµ ë°ì´í„° (MIMIC-III PPG):**
- ì†ê°€ë½ PPG ì„¼ì„œë¡œ ì§ì ‘ ì¸¡ì •
- 125 Hz ê³ í’ˆì§ˆ ìƒ˜í”Œë§
- í˜ˆë¥˜ ë³€í™” ì§ì ‘ ë°˜ì˜

**í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì¹´ë©”ë¼ rPPG):**
- ì–¼êµ´ í”¼ë¶€ì—ì„œ ê°„ì ‘ ì¶”ì¶œ
- 30 Hz â†’ ë³´ê°„ â†’ 125 Hz
- ì¡°ëª…/ì›€ì§ì„ ì•„í‹°íŒ©íŠ¸ í¬í•¨

**ê²°ê³¼:**
```
ì˜ˆì¸¡ê°’: SBP=2028 mmHg, DBP=946 mmHg (ë¹„ì •ìƒ)
ì •ìƒë²”ìœ„: SBP=90-140 mmHg, DBP=60-90 mmHg
```

#### ğŸŸ¡ Medium Issue 5: ë¦¬ìƒ˜í”Œë§ ë°©ì‹

**í˜„ì¬ êµ¬í˜„:**
```python
# ì„ í˜• ë³´ê°„
signal = np.interp(np.linspace(0, len(signal), 875), 
                   np.arange(len(signal)), signal)
```

**ë¬¸ì œì :**
- Nyquist ì£¼íŒŒìˆ˜ ê³ ë ¤ ì•ˆë¨
- ì•¨ë¦¬ì–´ì‹± ë°œìƒ ê°€ëŠ¥
- Anti-aliasing í•„í„° ì—†ìŒ

#### ğŸŸ¡ Medium Issue 6: ì›€ì§ì„/ì¡°ëª… ë³´ì • ì—†ìŒ

**í•„ìš”í•œ ë³´ì •:**
- ë¨¸ë¦¬ ì›€ì§ì„ ì¶”ì  ë° ë³´ìƒ
- ì¡°ëª… ë³€í™” ì •ê·œí™”
- í”¼ë¶€ ì˜ì—­ ë§ˆìŠ¤í‚¹

---

## 2. ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•

### 2.1 Phase 1: Quick Fix (1-2ì¼)

#### Step 1: Haar Cascade íŒŒë¼ë¯¸í„° ì¡°ì •
```python
# Before
faces = face_cascade.detectMultiScale(gray, 1.1, 4)

# After
faces = face_cascade.detectMultiScale(
    gray,
    scaleFactor=1.1,
    minNeighbors=8,      # 4 â†’ 8 (ê±°ì§“ ê°ì§€ ê°ì†Œ)
    minSize=(100, 100),  # ìµœì†Œ í¬ê¸° ì§€ì •
    maxSize=(400, 400)   # ìµœëŒ€ í¬ê¸° ì§€ì •
)
```

#### Step 2: ê¸°ë³¸ ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì¶”ê°€
```python
from scipy.signal import butter, filtfilt

def bandpass_filter(signal, lowcut=0.7, highcut=4.0, fs=30, order=4):
    """ì‹¬ë°•ìˆ˜ ë²”ìœ„ í•„í„° (0.7-4 Hz = 42-240 bpm)"""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)
```

#### Step 3: ê°€ì¥ í° ì–¼êµ´ë§Œ ì‚¬ìš© + ë‹¨ì¼ ë°•ìŠ¤
```python
def get_largest_face(faces):
    if len(faces) == 0:
        return None
    # ë©´ì  ê¸°ì¤€ ê°€ì¥ í° ì–¼êµ´
    largest = max(faces, key=lambda f: f[2] * f[3])
    return largest
```

#### Step 4: ResNet ëª¨ë¸ë¡œ ë³€ê²½
```python
# AlexNet â†’ ResNet (ë…¼ë¬¸ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥)
parser.add_argument('--model', type=str, 
                    default='data/resnet_ppg_nonmixed.h5')
```

**ì˜ˆìƒ íš¨ê³¼:** ì˜ˆì¸¡ê°’ì´ ì—¬ì „íˆ ë¶€ì •í™•í•˜ì§€ë§Œ, ë²”ìœ„ê°€ ì¤„ì–´ë“¦

---

### 2.2 Phase 2: ë…¼ë¬¸ êµ¬í˜„ (2-4ì£¼)

#### Step 1: POS ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

```python
import numpy as np
from scipy.signal import butter, filtfilt

class POSAlgorithm:
    """
    Wang et al. 2017 - Plane-Orthogonal-to-Skin Algorithm
    ë…¼ë¬¸: "Algorithmic Principles of Remote PPG"
    IEEE Trans. Biomed. Eng., vol. 64, no. 7, pp. 1479-1491, 2017
    """
    
    def __init__(self, fs=30, window_size=32):
        self.fs = fs
        self.window_size = window_size
    
    def extract_pulse(self, rgb_signals):
        """
        RGB ì‹ í˜¸ì—ì„œ í„ìŠ¤ ì‹ í˜¸ ì¶”ì¶œ
        
        Args:
            rgb_signals: (N, 3) array - R, G, B ì±„ë„ ì‹œê³„ì—´
            
        Returns:
            pulse: (N,) array - ì¶”ì¶œëœ í„ìŠ¤ ì‹ í˜¸
        """
        N = rgb_signals.shape[0]
        H = np.zeros(N)
        
        for t in range(self.window_size, N):
            # ìœˆë„ìš° ë‚´ RGB ì‹ í˜¸
            C = rgb_signals[t-self.window_size:t, :].T  # (3, window_size)
            
            # ì‹œê°„ ì •ê·œí™” (í‰ê· ìœ¼ë¡œ ë‚˜ëˆ”)
            mean_C = np.mean(C, axis=1, keepdims=True)
            C_norm = C / (mean_C + 1e-8)
            
            # POS íˆ¬ì˜ í–‰ë ¬
            # P = [[0, 1, -1], [-2, 1, 1]]
            S = np.array([
                C_norm[1, :] - C_norm[2, :],           # G - B
                -2*C_norm[0, :] + C_norm[1, :] + C_norm[2, :]  # -2R + G + B
            ])
            
            # í‘œì¤€í¸ì°¨ ë¹„ìœ¨ë¡œ ê²°í•©
            std_S0 = np.std(S[0, :])
            std_S1 = np.std(S[1, :])
            
            if std_S1 > 1e-8:
                alpha = std_S0 / std_S1
            else:
                alpha = 0
            
            # í„ìŠ¤ ì‹ í˜¸
            h = S[0, :] + alpha * S[1, :]
            
            # ìœˆë„ìš° ì¤‘ì‹¬ê°’ ì €ì¥
            H[t] = h[-1] - np.mean(h)
        
        return H
    
    def process_video(self, frames, face_detector):
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ rPPG ì‹ í˜¸ ì¶”ì¶œ
        
        Args:
            frames: list of BGR frames
            face_detector: ì–¼êµ´ ê°ì§€ê¸°
            
        Returns:
            pulse: ì¶”ì¶œëœ í„ìŠ¤ ì‹ í˜¸
        """
        rgb_signals = []
        
        for frame in frames:
            # ì–¼êµ´ ê°ì§€
            face_roi = face_detector.detect(frame)
            if face_roi is None:
                continue
            
            # í”¼ë¶€ ì˜ì—­ì—ì„œ RGB í‰ê·  ì¶”ì¶œ
            r_mean = np.mean(face_roi[:, :, 2])  # R
            g_mean = np.mean(face_roi[:, :, 1])  # G
            b_mean = np.mean(face_roi[:, :, 0])  # B
            
            rgb_signals.append([r_mean, g_mean, b_mean])
        
        rgb_signals = np.array(rgb_signals)
        
        # POS ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í„ìŠ¤ ì¶”ì¶œ
        pulse = self.extract_pulse(rgb_signals)
        
        # ë°´ë“œíŒ¨ìŠ¤ í•„í„°ë§
        pulse = self.bandpass_filter(pulse)
        
        return pulse
    
    def bandpass_filter(self, signal, lowcut=0.7, highcut=4.0):
        """ì‹¬ë°•ìˆ˜ ë²”ìœ„ í•„í„°ë§"""
        nyq = 0.5 * self.fs
        low = lowcut / nyq
        high = highcut / nyq
        b, a = butter(4, [low, high], btype='band')
        return filtfilt(b, a, signal)
```

#### Step 2: MediaPipe ì–¼êµ´ ê°ì§€ ë„ì…

```python
import mediapipe as mp
import cv2

class MediaPipeFaceDetector:
    def __init__(self, min_detection_confidence=0.7):
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=0,  # 0: 2m ì´ë‚´, 1: 5m ì´ë‚´
            min_detection_confidence=min_detection_confidence
        )
    
    def detect(self, frame):
        """ì–¼êµ´ ì˜ì—­ ë°˜í™˜"""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb)
        
        if not results.detections:
            return None
        
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì–¼êµ´
        detection = max(results.detections, 
                       key=lambda d: d.score[0])
        
        bbox = detection.location_data.relative_bounding_box
        h, w = frame.shape[:2]
        
        x = int(bbox.xmin * w)
        y = int(bbox.ymin * h)
        width = int(bbox.width * w)
        height = int(bbox.height * h)
        
        # ê²½ê³„ ì²´í¬
        x = max(0, x)
        y = max(0, y)
        width = min(width, w - x)
        height = min(height, h - y)
        
        return frame[y:y+height, x:x+width]
    
    def detect_with_landmarks(self, frame):
        """ì–¼êµ´ + ëœë“œë§ˆí¬ ë°˜í™˜ (í”¼ë¶€ ì˜ì—­ ì¶”ì¶œìš©)"""
        # MediaPipe Face Mesh ì‚¬ìš© ì‹œ ë” ì •ë°€í•œ í”¼ë¶€ ì˜ì—­ ì¶”ì¶œ ê°€ëŠ¥
        pass
```

#### Step 3: ì ì ˆí•œ ë¦¬ìƒ˜í”Œë§

```python
from scipy.signal import resample

def proper_resample(signal, original_fs, target_fs, target_length):
    """
    ì ì ˆí•œ ë¦¬ìƒ˜í”Œë§ (Anti-aliasing í¬í•¨)
    
    Args:
        signal: ì›ë³¸ ì‹ í˜¸
        original_fs: ì›ë³¸ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        target_fs: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        target_length: ëª©í‘œ ê¸¸ì´ (ìƒ˜í”Œ ìˆ˜)
    """
    # Anti-aliasing í•„í„°
    if target_fs < original_fs:
        nyq = 0.5 * original_fs
        cutoff = 0.5 * target_fs / nyq
        b, a = butter(8, cutoff, btype='low')
        signal = filtfilt(b, a, signal)
    
    # scipy resample (FFT ê¸°ë°˜)
    resampled = resample(signal, target_length)
    
    return resampled
```

#### Step 4: Transfer Learning / Fine-tuning

```python
# retrain_rppg_personalization.py í™œìš©
# PPGë¡œ í•™ìŠµëœ ëª¨ë¸ì„ rPPG ë°ì´í„°ë¡œ fine-tuning

python retrain_rppg_personalization.py \
    "experiment_name" \
    "data/rPPG-BP-UKL_rppg_7s.h5" \
    "results/" \
    "data/resnet_ppg_nonmixed.h5" \
    "checkpoints/"
```

---

### 2.3 Phase 3: 2026 ìµœì‹  ê¸°ìˆ  (4-8ì£¼)

[ì„¹ì…˜ 4ì—ì„œ ìƒì„¸ ì„¤ëª…]

---

## 3. ë…¼ë¬¸ ê¸°ë°˜ ì •í™•í•œ êµ¬í˜„

### 3.1 POS ì•Œê³ ë¦¬ì¦˜ ìˆ˜í•™ì  ë°°ê²½

**Wang et al. 2017 ë…¼ë¬¸ì˜ í•µì‹¬ ì›ë¦¬:**

í”¼ë¶€ ë°˜ì‚¬ ëª¨ë¸:
```
I(t) = I_s(t) + I_d(t)
     = specular reflection + diffuse reflection
```

í”¼ë¶€ìƒ‰ ë³€í™”:
```
C(t) = C_0 Â· (1 + p(t))
```
- `C(t)`: ì‹œê°„ tì—ì„œì˜ í”¼ë¶€ìƒ‰ (RGB)
- `C_0`: ê¸°ì € í”¼ë¶€ìƒ‰
- `p(t)`: í˜ˆë¥˜ ë³€í™”ì— ì˜í•œ ë¯¸ì„¸ ë³€ë™

**POS íˆ¬ì˜:**
```
S = P Â· C_n

ì—¬ê¸°ì„œ:
P = [0,  1, -1]    (ì²« ë²ˆì§¸ ì¶•)
    [-2, 1,  1]    (ë‘ ë²ˆì§¸ ì¶•)

C_n = C(t) / mean(C(t))  (ì‹œê°„ ì •ê·œí™”)
```

**í„ìŠ¤ ì‹ í˜¸ ì¶”ì¶œ:**
```
H = S_1 + (Ïƒ(S_1) / Ïƒ(S_2)) Â· S_2
```
- `Ïƒ()`: í‘œì¤€í¸ì°¨
- ë‘ ì§êµ ì„±ë¶„ì˜ ê°€ì¤‘ í•©

### 3.2 4ê°œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸

#### 3.2.1 AlexNet (1D ë³€í˜•)

```
ì…ë ¥: (875, 1) - 7ì´ˆ Ã— 125Hz

Layer 1: Conv1D(96, 11, stride=4) â†’ ReLU â†’ MaxPool(3, 2)
Layer 2: Conv1D(256, 5, padding='same') â†’ ReLU â†’ MaxPool(3, 2)
Layer 3: Conv1D(384, 3, padding='same') â†’ ReLU
Layer 4: Conv1D(384, 3, padding='same') â†’ ReLU
Layer 5: Conv1D(256, 3, padding='same') â†’ ReLU â†’ MaxPool(3, 2)
Layer 6: Flatten â†’ Dense(4096) â†’ ReLU â†’ Dropout(0.5)
Layer 7: Dense(4096) â†’ ReLU â†’ Dropout(0.5)
Layer 8: Dense(1, 'SBP') + Dense(1, 'DBP')

íŒŒë¼ë¯¸í„° ìˆ˜: ~60M
```

#### 3.2.2 ResNet50 (1D ë³€í˜•) - **ìµœê³  ì„±ëŠ¥**

```
ì…ë ¥: (875, 1)

Stage 0: Conv1D(64, 7, stride=2) â†’ BN â†’ ReLU â†’ MaxPool(3, 3)
Stage 1: ConvBlock(64,64,256) â†’ IdentityBlock Ã— 2
Stage 2: ConvBlock(128,128,512) â†’ IdentityBlock Ã— 3
Stage 3: ConvBlock(256,256,1024) â†’ IdentityBlock Ã— 5
Stage 4: ConvBlock(512,512,2048) â†’ IdentityBlock Ã— 2
ì¶œë ¥: AvgPool â†’ Flatten â†’ Dense(1, 'SBP') + Dense(1, 'DBP')

íŒŒë¼ë¯¸í„° ìˆ˜: ~25M
```

#### 3.2.3 LSTM (Bidirectional)

```
ì…ë ¥: (875, 1)

Layer 1: Conv1D(64, 5, padding='causal') â†’ ReLU
Layer 2: Bidirectional(LSTM(128, return_sequences=True))
Layer 3: Bidirectional(LSTM(128, return_sequences=True))
Layer 4: Bidirectional(LSTM(64, return_sequences=False))
Layer 5: Dense(512) â†’ ReLU
Layer 6: Dense(256) â†’ ReLU
Layer 7: Dense(128) â†’ ReLU
Layer 8: Dense(1, 'SBP') + Dense(1, 'DBP')

íŒŒë¼ë¯¸í„° ìˆ˜: ~3M
```

#### 3.2.4 Slapnicar (Spectro-Temporal)

```
ì…ë ¥: (875, 1)

ì‹œê°„ ë„ë©”ì¸ ë¶„ê¸°:
â”œâ”€â”€ PPG ì›ë³¸ â†’ SingleChannelResNet â†’ GRU(65) â†’ BN
â”œâ”€â”€ PPG 1ì°¨ ë¯¸ë¶„ â†’ SingleChannelResNet â†’ GRU(65) â†’ BN  (ì„ íƒì )
â””â”€â”€ PPG 2ì°¨ ë¯¸ë¶„ â†’ SingleChannelResNet â†’ GRU(65) â†’ BN  (ì„ íƒì )

ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¶„ê¸°:
â”œâ”€â”€ STFT(128, hop=64) â†’ Magnitude â†’ Dense(32) â†’ BN
â”œâ”€â”€ (ê° ë¯¸ë¶„ì— ëŒ€í•´ ë™ì¼)
â””â”€â”€ 

ë³‘í•©:
Concatenate([ì‹œê°„, ì£¼íŒŒìˆ˜]) â†’ Dense(32) â†’ Dropout(0.25)
â†’ Dense(32) â†’ Dropout(0.25)
â†’ Dense(1, 'SBP') + Dense(1, 'DBP')

íŒŒë¼ë¯¸í„° ìˆ˜: ~5M
```

### 3.3 ë…¼ë¬¸ì˜ ì‹ í˜¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```
[ì›ë³¸ ë°ì´í„°]
     â†“
[ì „ì²˜ë¦¬]
â”œâ”€â”€ Butterworth Bandpass Filter (0.5-8 Hz)
â”œâ”€â”€ Z-score Normalization
â””â”€â”€ ìœˆë„ìš° ë¶„í•  (7ì´ˆ, 50% ì˜¤ë²„ë©)
     â†“
[í’ˆì§ˆ ê²€ì‚¬]
â”œâ”€â”€ SNR > -7 dB
â”œâ”€â”€ SBP: 75-165 mmHg
â”œâ”€â”€ DBP: 40-80 mmHg
â””â”€â”€ HR: 50-140 bpm
     â†“
[ë°ì´í„°ì…‹ ë¶„í• ]
â”œâ”€â”€ Subject-based split (Non-mixed)
â”œâ”€â”€ Train: 3750 subjects, 1M samples
â”œâ”€â”€ Val: 625 subjects, 250K samples
â””â”€â”€ Test: 625 subjects, 250K samples
     â†“
[í•™ìŠµ]
â”œâ”€â”€ Optimizer: Adam (lr=0.001)
â”œâ”€â”€ Loss: MSE
â”œâ”€â”€ Early Stopping (patience=10)
â””â”€â”€ Checkpoint: Best validation loss
```

---

## 4. 2026ë…„ ìµœì‹  ë°©ë²•ë¡ 

### 4.1 Vision Transformer (ViT) for rPPG

**PhysFormer (2022-2024 ë°œì „):**

```python
class PhysFormer(nn.Module):
    """
    Transformer ê¸°ë°˜ End-to-End rPPG ì¶”ì¶œ
    ë¹„ë””ì˜¤ â†’ ì§ì ‘ BP ì˜ˆì¸¡
    """
    def __init__(self, img_size=128, patch_size=4, 
                 embed_dim=768, depth=12, num_heads=12):
        super().__init__()
        
        # Temporal Difference Convolution
        self.stem = nn.Sequential(
            nn.Conv3d(3, 64, (1, 5, 5), padding=(0, 2, 2)),
            nn.BatchNorm3d(64),
            nn.ReLU(),
            nn.Conv3d(64, 64, (3, 3, 3), padding=(1, 1, 1)),
        )
        
        # Patch Embedding
        self.patch_embed = PatchEmbed3D(
            img_size=img_size,
            patch_size=patch_size,
            embed_dim=embed_dim
        )
        
        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(
                embed_dim, num_heads, 
                mlp_ratio=4.0, drop=0.1
            ) for _ in range(depth)
        ])
        
        # Temporal Attention
        self.temporal_attn = TemporalAttention(embed_dim)
        
        # BP Prediction Head
        self.bp_head = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Linear(embed_dim, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 2)  # SBP, DBP
        )
    
    def forward(self, x):
        # x: (B, C, T, H, W)
        x = self.stem(x)
        x = self.patch_embed(x)
        
        for block in self.blocks:
            x = block(x)
        
        x = self.temporal_attn(x)
        bp = self.bp_head(x)
        
        return bp
```

### 4.2 Self-Supervised Learning

**Contrastive Learning for rPPG:**

```python
class ContrastiveRPPG(nn.Module):
    """
    ë ˆì´ë¸” ì—†ì´ rPPG íŠ¹ì§• í•™ìŠµ
    ê°™ì€ ì‚¬ëŒì˜ ë‹¤ë¥¸ ì‹œê°„ëŒ€ = positive pair
    ë‹¤ë¥¸ ì‚¬ëŒ = negative pair
    """
    def __init__(self, encoder, projection_dim=128):
        super().__init__()
        self.encoder = encoder
        self.projector = nn.Sequential(
            nn.Linear(encoder.output_dim, 512),
            nn.ReLU(),
            nn.Linear(512, projection_dim)
        )
        self.temperature = 0.07
    
    def contrastive_loss(self, z_i, z_j):
        """NT-Xent Loss"""
        batch_size = z_i.shape[0]
        z = torch.cat([z_i, z_j], dim=0)
        
        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)
        sim = sim / self.temperature
        
        # Positive pairs: (i, i+batch_size) and (i+batch_size, i)
        labels = torch.cat([
            torch.arange(batch_size, 2*batch_size),
            torch.arange(batch_size)
        ]).to(z.device)
        
        loss = F.cross_entropy(sim, labels)
        return loss
```

### 4.3 Multi-Task Learning

**ë™ì‹œ ì˜ˆì¸¡: BP + HR + SpO2**

```python
class MultiTaskBPModel(nn.Module):
    """
    ì—¬ëŸ¬ ìƒì²´ì‹ í˜¸ ë™ì‹œ ì˜ˆì¸¡ìœ¼ë¡œ íŠ¹ì§• ê³µìœ 
    """
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        
        # Shared features
        self.shared = nn.Sequential(
            nn.Linear(backbone.output_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Task-specific heads
        self.bp_head = nn.Linear(512, 2)    # SBP, DBP
        self.hr_head = nn.Linear(512, 1)    # Heart Rate
        self.spo2_head = nn.Linear(512, 1)  # SpO2
        
    def forward(self, x):
        features = self.backbone(x)
        shared = self.shared(features)
        
        bp = self.bp_head(shared)
        hr = self.hr_head(shared)
        spo2 = self.spo2_head(shared)
        
        return {
            'bp': bp,
            'hr': hr,
            'spo2': spo2
        }
    
    def compute_loss(self, pred, target, weights={'bp': 1.0, 'hr': 0.3, 'spo2': 0.3}):
        loss = 0
        for task, weight in weights.items():
            loss += weight * F.mse_loss(pred[task], target[task])
        return loss
```

### 4.4 Domain Adaptation (PPG â†’ rPPG)

```python
class DomainAdaptationBP(nn.Module):
    """
    PPG ë„ë©”ì¸ì—ì„œ í•™ìŠµ â†’ rPPG ë„ë©”ì¸ìœ¼ë¡œ ì ì‘
    Adversarial Training ì‚¬ìš©
    """
    def __init__(self, feature_extractor, bp_predictor):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.bp_predictor = bp_predictor
        
        # Domain discriminator
        self.domain_classifier = nn.Sequential(
            GradientReversal(lambda_=1.0),  # Gradient Reversal Layer
            nn.Linear(feature_extractor.output_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x, return_domain=False):
        features = self.feature_extractor(x)
        bp = self.bp_predictor(features)
        
        if return_domain:
            domain = self.domain_classifier(features)
            return bp, domain
        return bp
```

### 4.5 Diffusion Models for Signal Denoising

```python
class DiffusionDenoiser(nn.Module):
    """
    Diffusion Modelë¡œ rPPG ì‹ í˜¸ ë…¸ì´ì¦ˆ ì œê±°
    """
    def __init__(self, signal_dim=875, time_embed_dim=128):
        super().__init__()
        
        self.time_embed = nn.Sequential(
            SinusoidalPositionEmbeddings(time_embed_dim),
            nn.Linear(time_embed_dim, time_embed_dim * 4),
            nn.GELU(),
            nn.Linear(time_embed_dim * 4, time_embed_dim)
        )
        
        self.unet = UNet1D(
            in_channels=1,
            out_channels=1,
            time_embed_dim=time_embed_dim
        )
    
    def forward(self, x_noisy, t):
        t_embed = self.time_embed(t)
        noise_pred = self.unet(x_noisy, t_embed)
        return noise_pred
    
    @torch.no_grad()
    def denoise(self, x_noisy, num_steps=50):
        """DDPM ì—­ê³¼ì •ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°"""
        for t in reversed(range(num_steps)):
            t_tensor = torch.full((x_noisy.shape[0],), t)
            noise_pred = self.forward(x_noisy, t_tensor)
            x_noisy = self.ddpm_step(x_noisy, noise_pred, t)
        return x_noisy
```

### 4.6 Real-time Optimization

```python
# ONNX ë³€í™˜ ë° ìµœì í™”
import onnx
import onnxruntime as ort

def export_to_onnx(model, sample_input, output_path):
    """PyTorch â†’ ONNX ë³€í™˜"""
    torch.onnx.export(
        model,
        sample_input,
        output_path,
        input_names=['input'],
        output_names=['sbp', 'dbp'],
        dynamic_axes={'input': {0: 'batch_size'}},
        opset_version=13
    )

def optimize_onnx(input_path, output_path):
    """ONNX ëª¨ë¸ ìµœì í™”"""
    from onnxruntime.transformers import optimizer
    optimized = optimizer.optimize_model(
        input_path,
        model_type='bert',  # transformer êµ¬ì¡°
        num_heads=12,
        hidden_size=768
    )
    optimized.save_model_to_file(output_path)

# TensorRT ë³€í™˜ (NVIDIA GPUìš©)
# trtexec --onnx=model.onnx --saveEngine=model.trt --fp16
```

---

## 5. êµ¬í˜„ ë¡œë“œë§µ

### 5.1 ì „ì²´ ì¼ì • (9ì£¼)

```
Week 1-2: Phase 1 (Quick Fix)
â”œâ”€â”€ Day 1-2: í™˜ê²½ ì„¤ì • ë° ê¸°ì¡´ ì½”ë“œ ë¶„ì„
â”œâ”€â”€ Day 3-4: Haar Cascade íŒŒë¼ë¯¸í„° ì¡°ì •
â”œâ”€â”€ Day 5-7: ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì¶”ê°€
â”œâ”€â”€ Day 8-10: MediaPipe ê¸°ë³¸ í†µí•©
â””â”€â”€ Day 11-14: ResNet ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ê¸°ë³¸ í‰ê°€

Week 3-5: Phase 2 (ë…¼ë¬¸ êµ¬í˜„)
â”œâ”€â”€ Week 3: POS ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµ¬í˜„
â”‚   â”œâ”€â”€ RGB ì¶”ì¶œ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ ì •ê·œí™” ë° íˆ¬ì˜
â”‚   â””â”€â”€ í„ìŠ¤ ì‹ í˜¸ ì¶”ì¶œ
â”œâ”€â”€ Week 4: ì‹ í˜¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ ì ì ˆí•œ ë¦¬ìƒ˜í”Œë§
â”‚   â”œâ”€â”€ SNR ê³„ì‚° ë° í’ˆì§ˆ í•„í„°
â”‚   â””â”€â”€ ì „ì²˜ë¦¬ í†µí•©
â””â”€â”€ Week 5: Transfer Learning
    â”œâ”€â”€ rPPG ë°ì´í„°ì…‹ ì¤€ë¹„
    â”œâ”€â”€ Fine-tuning ì‹¤í–‰
    â””â”€â”€ ì„±ëŠ¥ í‰ê°€

Week 6-8: Phase 3 (ìµœì‹  ê¸°ìˆ )
â”œâ”€â”€ Week 6: Transformer ê¸°ë°˜ ëª¨ë¸
â”‚   â”œâ”€â”€ PhysFormer êµ¬ì¡° êµ¬í˜„
â”‚   â””â”€â”€ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
â”œâ”€â”€ Week 7: Self-Supervised Pre-training
â”‚   â”œâ”€â”€ Contrastive Learning êµ¬í˜„
â”‚   â””â”€â”€ Pre-training ì‹¤í–‰
â””â”€â”€ Week 8: Multi-Task & Domain Adaptation
    â”œâ”€â”€ Multi-Task í—¤ë“œ ì¶”ê°€
    â””â”€â”€ Domain Adaptation ì ìš©

Week 9: ìµœì í™” ë° ë°°í¬
â”œâ”€â”€ Day 1-3: ONNX/TensorRT ë³€í™˜
â”œâ”€â”€ Day 4-5: Real-time ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â””â”€â”€ Day 6-7: ë¬¸ì„œí™” ë° ìµœì¢… ì •ë¦¬
```

### 5.2 ì£¼ìš” ë§ˆì¼ìŠ¤í†¤

| Week | ëª©í‘œ | ì˜ˆìƒ ê²°ê³¼ |
|------|------|----------|
| 2 | Quick Fix ì™„ë£Œ | MAE ê°œì„  (ë¹„ì •ìƒ â†’ 100+ mmHg) |
| 5 | ë…¼ë¬¸ êµ¬í˜„ ì™„ë£Œ | MAE ~16 mmHg (ë…¼ë¬¸ ìˆ˜ì¤€) |
| 8 | ìµœì‹  ê¸°ìˆ  ì ìš© | MAE ~10 mmHg |
| 9 | ìµœì í™” ì™„ë£Œ | Real-time (>30 FPS) |

---

## 6. ì„±ëŠ¥ ë¹„êµí‘œ

### 6.1 ë°©ë²•ë³„ ì˜ˆìƒ ì„±ëŠ¥

| êµ¬í˜„ ë‹¨ê³„ | SBP MAE (mmHg) | DBP MAE (mmHg) | ì¶”ë¡  ì‹œê°„ | êµ¬í˜„ ë‚œì´ë„ |
|-----------|---------------|---------------|-----------|-------------|
| í˜„ì¬ (Green í‰ê· ) | ~2000 (ë¹„ì •ìƒ) | ~900 (ë¹„ì •ìƒ) | <10ms | âœ… ì™„ë£Œ |
| Quick Fix | ~100-200 | ~50-100 | <20ms | â­ ì‰¬ì›€ |
| ë…¼ë¬¸ êµ¬í˜„ (ResNet) | **16.4** | **8.5** | ~50ms | â­â­â­ ë³´í†µ |
| ë…¼ë¬¸ + Fine-tuning | 12-14 | 6-7 | ~50ms | â­â­â­ ë³´í†µ |
| PhysFormer | 10-12 | 5-6 | ~100ms | â­â­â­â­ ì–´ë ¤ì›€ |
| Multi-Task + DA | **8-10** | **4-5** | ~100ms | â­â­â­â­â­ ë§¤ìš° ì–´ë ¤ì›€ |

### 6.2 ë…¼ë¬¸ì˜ ì‹¤ì œ ê²°ê³¼ (Non-mixed Dataset)

| ëª¨ë¸ | SBP MAE | DBP MAE | ë¹„ê³  |
|------|---------|---------|------|
| Mean Regressor | 20.2 | 10.7 | ê¸°ì¤€ì„  |
| AlexNet | 17.1 | 8.8 | |
| **ResNet** | **16.4** | **8.5** | ìµœê³  ì„±ëŠ¥ |
| LSTM | 17.6 | 9.0 | |
| Slapnicar | 18.3 | 9.4 | |

### 6.3 rPPG Fine-tuning í›„ ê²°ê³¼

| ì¡°ê±´ | SBP MAE | DBP MAE |
|------|---------|---------|
| PPG ëª¨ë¸ (Fine-tuning ì „) | 28.9 | 15.2 |
| rPPG Fine-tuning í›„ | 14.1 | 8.3 |
| + Personalization (first 20%) | **12.7** | **7.1** |

---

## 7. ì½”ë“œ ì˜ˆì œ

### 7.1 ì™„ì „í•œ POS ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

```python
"""
pos_algorithm.py - Wang et al. 2017 POS ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµ¬í˜„
"""

import numpy as np
from scipy.signal import butter, filtfilt, find_peaks
from scipy.fft import fft, fftfreq

class POSExtractor:
    """
    Plane-Orthogonal-to-Skin (POS) rPPG ì¶”ì¶œê¸°
    """
    
    def __init__(self, fs=30, window_size=1.6):
        """
        Args:
            fs: ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
            window_size: ìœˆë„ìš° í¬ê¸° (ì´ˆ)
        """
        self.fs = fs
        self.window_samples = int(window_size * fs)
        
    def extract_rgb_signals(self, frames, face_detector):
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ RGB ì‹œê³„ì—´ ì¶”ì¶œ
        
        Args:
            frames: BGR í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
            face_detector: ì–¼êµ´ ê°ì§€ ê°ì²´
            
        Returns:
            rgb: (N, 3) RGB ì‹ í˜¸
        """
        rgb_signals = []
        
        for frame in frames:
            roi = face_detector.detect(frame)
            
            if roi is not None and roi.size > 0:
                # í”¼ë¶€ ì˜ì—­ ë§ˆìŠ¤í‚¹ (ì„ íƒì‚¬í•­)
                skin_mask = self._get_skin_mask(roi)
                
                if skin_mask.sum() > 100:  # ìµœì†Œ í”½ì…€ ìˆ˜
                    r = np.mean(roi[:,:,2][skin_mask])
                    g = np.mean(roi[:,:,1][skin_mask])
                    b = np.mean(roi[:,:,0][skin_mask])
                else:
                    r = np.mean(roi[:,:,2])
                    g = np.mean(roi[:,:,1])
                    b = np.mean(roi[:,:,0])
                    
                rgb_signals.append([r, g, b])
            else:
                # ì´ì „ ê°’ ì‚¬ìš© ë˜ëŠ” ë³´ê°„
                if rgb_signals:
                    rgb_signals.append(rgb_signals[-1])
                    
        return np.array(rgb_signals)
    
    def _get_skin_mask(self, roi):
        """HSV ê¸°ë°˜ í”¼ë¶€ìƒ‰ ë§ˆìŠ¤í¬"""
        import cv2
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # í”¼ë¶€ìƒ‰ ë²”ìœ„ (HSV)
        lower = np.array([0, 20, 70])
        upper = np.array([20, 255, 255])
        
        mask = cv2.inRange(hsv, lower, upper)
        return mask > 0
    
    def pos_algorithm(self, rgb):
        """
        POS ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í„ìŠ¤ ì‹ í˜¸ ì¶”ì¶œ
        
        Args:
            rgb: (N, 3) RGB ì‹œê³„ì—´
            
        Returns:
            pulse: (N,) í„ìŠ¤ ì‹ í˜¸
        """
        N = rgb.shape[0]
        l = self.window_samples
        H = np.zeros(N)
        
        for t in range(l, N):
            # ìœˆë„ìš° ë‚´ RGB
            C = rgb[t-l:t, :].T  # (3, l)
            
            # ì‹œê°„ ì •ê·œí™”
            mean_C = np.mean(C, axis=1, keepdims=True)
            C_n = C / (mean_C + 1e-10)
            
            # POS íˆ¬ì˜
            S = np.array([
                C_n[1] - C_n[2],              # G - B
                C_n[1] + C_n[2] - 2*C_n[0]    # G + B - 2R
            ])
            
            # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ê²°í•©
            std1 = np.std(S[0])
            std2 = np.std(S[1])
            
            alpha = std1 / (std2 + 1e-10)
            
            # ìœˆë„ìš° ë‚´ í„ìŠ¤ ì‹ í˜¸
            h = S[0] + alpha * S[1]
            
            # ì¤‘ì²©-ê°€ì‚° ë°©ì‹
            H[t-l:t] += (h - np.mean(h))
        
        return H
    
    def bandpass_filter(self, signal, lowcut=0.7, highcut=4.0, order=4):
        """
        Butterworth ë°´ë“œíŒ¨ìŠ¤ í•„í„°
        
        Args:
            signal: ì…ë ¥ ì‹ í˜¸
            lowcut: í•˜í•œ ì£¼íŒŒìˆ˜ (Hz)
            highcut: ìƒí•œ ì£¼íŒŒìˆ˜ (Hz)
            
        Returns:
            filtered: í•„í„°ë§ëœ ì‹ í˜¸
        """
        nyq = 0.5 * self.fs
        low = lowcut / nyq
        high = highcut / nyq
        
        # ê²½ê³„ ì¡°ê±´ í™•ì¸
        if low <= 0:
            low = 0.01
        if high >= 1:
            high = 0.99
            
        b, a = butter(order, [low, high], btype='band')
        
        # ì‹ í˜¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ í•„í„°ë§ ê±´ë„ˆëœ€
        if len(signal) < 3 * max(len(b), len(a)):
            return signal
            
        return filtfilt(b, a, signal)
    
    def estimate_heart_rate(self, pulse_signal):
        """
        í„ìŠ¤ ì‹ í˜¸ì—ì„œ ì‹¬ë°•ìˆ˜ ì¶”ì •
        
        Returns:
            hr: ì‹¬ë°•ìˆ˜ (bpm)
        """
        # FFT ê¸°ë°˜ ì¶”ì •
        n = len(pulse_signal)
        freq = fftfreq(n, 1/self.fs)
        fft_vals = np.abs(fft(pulse_signal))
        
        # 0.7-4 Hz ë²”ìœ„
        valid_idx = (freq > 0.7) & (freq < 4.0)
        
        if not np.any(valid_idx):
            return 60  # ê¸°ë³¸ê°’
            
        peak_freq = freq[valid_idx][np.argmax(fft_vals[valid_idx])]
        hr = peak_freq * 60  # Hz â†’ bpm
        
        return hr
    
    def extract(self, frames, face_detector):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            frames: BGR í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
            face_detector: ì–¼êµ´ ê°ì§€ ê°ì²´
            
        Returns:
            pulse: ì¶”ì¶œëœ í„ìŠ¤ ì‹ í˜¸
            hr: ì¶”ì • ì‹¬ë°•ìˆ˜
        """
        # 1. RGB ì¶”ì¶œ
        rgb = self.extract_rgb_signals(frames, face_detector)
        
        if len(rgb) < self.window_samples * 2:
            raise ValueError("í”„ë ˆì„ ìˆ˜ ë¶€ì¡±")
        
        # 2. POS ì•Œê³ ë¦¬ì¦˜
        pulse = self.pos_algorithm(rgb)
        
        # 3. ë°´ë“œíŒ¨ìŠ¤ í•„í„°
        pulse = self.bandpass_filter(pulse)
        
        # 4. ì •ê·œí™”
        pulse = (pulse - np.mean(pulse)) / (np.std(pulse) + 1e-10)
        
        # 5. ì‹¬ë°•ìˆ˜ ì¶”ì •
        hr = self.estimate_heart_rate(pulse)
        
        return pulse, hr


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import cv2
    
    # MediaPipe ì–¼êµ´ ê°ì§€ê¸°
    class SimpleFaceDetector:
        def __init__(self):
            self.cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            
        def detect(self, frame):
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.cascade.detectMultiScale(gray, 1.1, 8, minSize=(100, 100))
            
            if len(faces) == 0:
                return None
                
            x, y, w, h = max(faces, key=lambda f: f[2]*f[3])
            return frame[y:y+h, x:x+w]
    
    # ì¹´ë©”ë¼ ìº¡ì²˜
    cap = cv2.VideoCapture(0)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    
    detector = SimpleFaceDetector()
    extractor = POSExtractor(fs=fps)
    
    frames = []
    duration = 7  # 7ì´ˆ
    
    print(f"Capturing {duration} seconds of video...")
    
    while len(frames) < duration * fps:
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
            cv2.imshow('Capture', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    cap.release()
    cv2.destroyAllWindows()
    
    # ì‹ í˜¸ ì¶”ì¶œ
    pulse, hr = extractor.extract(frames, detector)
    
    print(f"Estimated Heart Rate: {hr:.1f} bpm")
    print(f"Pulse signal length: {len(pulse)}")
```

### 7.2 ê°œì„ ëœ ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
"""
improved_camera_rppg.py - Quick Fix ì ìš© ë²„ì „
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

import cv2
import numpy as np
from scipy.signal import butter, filtfilt, resample
import tensorflow as tf
import tensorflow.keras as ks
from kapre import STFT, Magnitude, MagnitudeToDecibel


def bandpass_filter(signal, lowcut=0.7, highcut=4.0, fs=30, order=4):
    """ë°´ë“œíŒ¨ìŠ¤ í•„í„°"""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, signal)


def get_largest_face(faces):
    """ê°€ì¥ í° ì–¼êµ´ ë°˜í™˜"""
    if len(faces) == 0:
        return None
    return max(faces, key=lambda f: f[2] * f[3])


def load_model(model_path):
    """ëª¨ë¸ ë¡œë“œ"""
    dependencies = {
        'ReLU': ks.layers.ReLU,
        'STFT': STFT,
        'Magnitude': Magnitude,
        'MagnitudeToDecibel': MagnitudeToDecibel
    }
    return ks.models.load_model(model_path, custom_objects=dependencies)


def main():
    # ì„¤ì •
    MODEL_PATH = 'data/resnet_ppg_nonmixed.h5'  # ResNet ì‚¬ìš©
    CAMERA_ID = 0
    DURATION = 7  # ì´ˆ
    TARGET_SAMPLES = 875
    
    print("="*60)
    print("ê°œì„ ëœ rPPG í˜ˆì•• ì˜ˆì¸¡")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nëª¨ë¸ ë¡œë”©...")
    model = load_model(MODEL_PATH)
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # ì¹´ë©”ë¼ ì´ˆê¸°í™”
    cap = cv2.VideoCapture(CAMERA_ID, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    print(f"âœ“ ì¹´ë©”ë¼ ì´ˆê¸°í™” (FPS: {fps})")
    
    # ì–¼êµ´ ê°ì§€ê¸°
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    # ì‹ í˜¸ ë²„í¼
    signal_buffer = []
    target_frames = int(DURATION * fps)
    
    print(f"\n{DURATION}ì´ˆ ë™ì•ˆ ì‹ í˜¸ ìˆ˜ì§‘ ì‹œì‘...")
    print("ì–¼êµ´ì„ ì¹´ë©”ë¼ì— ë§ì¶°ì£¼ì„¸ìš”. 'q'ë¥¼ ëˆŒëŸ¬ ì·¨ì†Œ.")
    
    while len(signal_buffer) < target_frames:
        ret, frame = cap.read()
        if not ret:
            continue
        
        # ì–¼êµ´ ê°ì§€ (ê°œì„ ëœ íŒŒë¼ë¯¸í„°)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.1,
            minNeighbors=8,        # 4 â†’ 8
            minSize=(100, 100),
            maxSize=(400, 400)
        )
        
        # ê°€ì¥ í° ì–¼êµ´ë§Œ ì‚¬ìš©
        face = get_largest_face(faces)
        
        if face is not None:
            x, y, w, h = face
            roi = frame[y:y+h, x:x+w]
            
            # Green ì±„ë„ í‰ê·  (ì¶”í›„ POSë¡œ êµì²´)
            green_mean = np.mean(roi[:, :, 1])
            signal_buffer.append(green_mean)
            
            # ë‹¨ì¼ ë°•ìŠ¤ë§Œ í‘œì‹œ
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # ì§„í–‰ ìƒí™©
            progress = len(signal_buffer) / target_frames * 100
            cv2.putText(frame, f"Progress: {progress:.1f}%", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.imshow('rPPG Capture', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("ì·¨ì†Œë¨")
            cap.release()
            cv2.destroyAllWindows()
            return
    
    cap.release()
    cv2.destroyAllWindows()
    
    print(f"âœ“ {len(signal_buffer)} ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # ì‹ í˜¸ ì²˜ë¦¬
    signal = np.array(signal_buffer)
    
    # 1. ë°´ë“œíŒ¨ìŠ¤ í•„í„°
    signal = bandpass_filter(signal, fs=fps)
    print("âœ“ ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì ìš© (0.7-4 Hz)")
    
    # 2. ì •ê·œí™”
    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-10)
    print("âœ“ ì •ê·œí™” ì™„ë£Œ")
    
    # 3. ë¦¬ìƒ˜í”Œë§ (scipy resample ì‚¬ìš©)
    signal = resample(signal, TARGET_SAMPLES)
    print(f"âœ“ ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ ({len(signal_buffer)} â†’ {TARGET_SAMPLES})")
    
    # ì˜ˆì¸¡
    input_data = signal.reshape(1, TARGET_SAMPLES, 1)
    prediction = model.predict(input_data, verbose=0)
    
    # ê²°ê³¼ íŒŒì‹±
    if isinstance(prediction, list):
        sbp = float(prediction[0][0])
        dbp = float(prediction[1][0])
    else:
        sbp = float(prediction[0, 0])
        dbp = float(prediction[0, 1])
    
    print("\n" + "="*60)
    print("ì˜ˆì¸¡ ê²°ê³¼")
    print("="*60)
    print(f"ìˆ˜ì¶•ê¸° í˜ˆì•• (SBP): {sbp:.1f} mmHg")
    print(f"ì´ì™„ê¸° í˜ˆì•• (DBP): {dbp:.1f} mmHg")
    
    # ê²½ê³ 
    if sbp > 200 or sbp < 50 or dbp > 150 or dbp < 30:
        print("\nâš ï¸  ê²½ê³ : ì˜ˆì¸¡ê°’ì´ ì •ìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        print("   ì´ëŠ” rPPG ì‹ í˜¸ í’ˆì§ˆ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   POS ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
```

---

## 8. ì°¸ê³  ìë£Œ

### 8.1 í•µì‹¬ ë…¼ë¬¸

1. **Schrumpf et al. 2021** - ë³¸ í”„ë¡œì íŠ¸ ê¸°ë°˜ ë…¼ë¬¸
   - "Assessment of Non-Invasive Blood Pressure Prediction from PPG and rPPG Signals Using Deep Learning"
   - Sensors 2021, 21(18), 6022

2. **Wang et al. 2017** - POS ì•Œê³ ë¦¬ì¦˜
   - "Algorithmic Principles of Remote PPG"
   - IEEE Trans. Biomed. Eng., vol. 64, no. 7, pp. 1479-1491

3. **SlapniÄar et al. 2019** - Spectro-temporal ëª¨ë¸
   - "Blood Pressure Estimation from Photoplethysmogram Using a Spectro-Temporal Deep Neural Network"
   - Sensors 2019, 19(15), 3420

### 8.2 ìµœì‹  ë…¼ë¬¸ (2023-2026)

1. **PhysFormer** (2022)
   - "PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer"
   - CVPR 2022

2. **EfficientPhys** (2023)
   - "EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Cardiac Measurement"
   - WACV 2023

3. **Contrast-Phys** (2023)
   - "Contrast-Phys: Self-Supervised Learning for Remote Physiological Measurement"
   - ICCV 2023

### 8.3 ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸

1. **pyVHR** - Python Video Heart Rate
   - https://github.com/phuselab/pyVHR
   - POS, CHROM, ICA ë“± ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

2. **rPPG-Toolbox**
   - https://github.com/ubicomplab/rPPG-Toolbox
   - ë²¤ì¹˜ë§ˆí¬ ë° í‰ê°€ ë„êµ¬

3. **PhysNet**
   - End-to-end rPPG ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬

### 8.4 ë°ì´í„°ì…‹

1. **MIMIC-III** - PPG ë°ì´í„°
   - https://physionet.org/content/mimiciii/

2. **UBFC-rPPG** - rPPG ë²¤ì¹˜ë§ˆí¬
   - https://sites.google.com/view/yaboromance/ubfc-rppg

3. **PURE** - rPPG ë°ì´í„°ì…‹
   - ë‹¤ì–‘í•œ ì›€ì§ì„ ì¡°ê±´ í¬í•¨

---

## ğŸ“ ë©´ì±… ì¡°í•­

âš ï¸ **ì¤‘ìš” ê³µì§€:**
- ì´ ë¬¸ì„œì˜ êµ¬í˜„ì€ **êµìœ¡ ë° ì—°êµ¬ ëª©ì **ì…ë‹ˆë‹¤
- **ì˜ë£Œ ì§„ë‹¨ì— ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”**
- ì‹¤ì œ í˜ˆì•• ì¸¡ì •ì´ í•„ìš”í•˜ë©´ **ì¸ì¦ëœ ì˜ë£Œ ê¸°ê¸°**ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
- ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ì˜ë£Œì  ê²°ì •ì˜ ê·¼ê±°ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤

---

**ë¬¸ì„œ ì‘ì„±:** 2026-01-19  
**ë²„ì „:** 1.0  
**ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì˜ˆì •:** Phase 1 êµ¬í˜„ ì™„ë£Œ í›„
